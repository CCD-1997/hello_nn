{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network\n",
    "###### iris classification using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('iris-data.txt', <http.client.HTTPMessage at 0x7f1c06a00f98>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", \n",
    "    \"iris-data.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seed random-generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_ratio = 0.8\n",
    "\n",
    "tmp_list = []\n",
    "tmp_set = set()\n",
    "features = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text-file to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"iris-data.txt\") as f:\n",
    "    for line in f.readlines():\n",
    "        if not line.isspace():\n",
    "            tmp_list.append(line)\n",
    "\n",
    "    random.shuffle(tmp_list)\n",
    "\n",
    "for line in tmp_list:\n",
    "    split_line = line.strip().split(',')\n",
    "    length_line = len(split_line)\n",
    "\n",
    "    for i in range(length_line - 1):\n",
    "        split_line[i] = float(split_line[i])\n",
    "\n",
    "    label = split_line[length_line - 1]\n",
    "    tmp_set.add(label)\n",
    "\n",
    "    features.append(split_line[:length_line - 1])\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_val = max([item for i in features for item in i])\n",
    "min_val = min([item for i in features for item in i])\n",
    "\n",
    "for i in range(len(features)):\n",
    "    for j in range(len(features[0])):\n",
    "        features[i][j] = (features[i][j] - min_val) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_list = list(tmp_set)\n",
    "for i in range(len(labels)):\n",
    "    labels[i] = tmp_list.index(labels[i])\n",
    "\n",
    "label_idx = np.array(labels)\n",
    "labels = np.zeros((len(labels), len(tmp_list)))\n",
    "labels[np.arange(len(labels)), label_idx] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split into train-test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train = np.array(features[:int(train_test_ratio * len(features))])\n",
    "features_test = np.array(features[int(train_test_ratio * len(features)):])\n",
    "\n",
    "labels_train = labels[:int(train_test_ratio * len(labels))]\n",
    "labels_test = labels[int(train_test_ratio * len(labels)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_input_layers = len(features_test[0])\n",
    "n_hidden_layers_1 = 5\n",
    "n_output_layers = len(tmp_list)\n",
    "\n",
    "learning_rate = 0.1\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input/output placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, n_input_layers])\n",
    "Y = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_1 = {\n",
    "    'weights': tf.Variable(tf.random_normal([n_input_layers, n_hidden_layers_1], stddev=0.1)),\n",
    "    'biases': tf.Variable(tf.random_normal([n_hidden_layers_1], stddev=0.1))\n",
    "}\n",
    "layer_op = {\n",
    "    'weights': tf.Variable(tf.random_normal([n_hidden_layers_1, n_output_layers], stddev=0.1)),\n",
    "    'biases': tf.Variable(tf.random_normal([n_output_layers], stddev=0.1))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_l1 = tf.add(tf.matmul(X, layer_1['weights']), layer_1['biases'])\n",
    "l1 = tf.nn.sigmoid(h_l1)\n",
    "\n",
    "h_l2 = tf.add(tf.matmul(l1, layer_op['weights']), layer_op['biases'])\n",
    "op = tf.nn.sigmoid(h_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mean-squared error\n",
    "err = tf.reduce_mean(0.5 * tf.square(op - Y))\n",
    "\n",
    "# adam-optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** Train ***********\n",
      "Epoch: 0  Error: 0.123612\n",
      "Epoch: 10  Error: 0.109897\n",
      "Epoch: 20  Error: 0.0952621\n",
      "Epoch: 30  Error: 0.0654739\n",
      "Epoch: 40  Error: 0.0514822\n",
      "Epoch: 50  Error: 0.0461155\n",
      "Epoch: 60  Error: 0.0424945\n",
      "Epoch: 70  Error: 0.0380616\n",
      "Epoch: 80  Error: 0.030796\n",
      "Epoch: 90  Error: 0.0219716\n",
      "*********** Test ***********\n",
      "Accuracy: 93.3333337307\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    print(\"*********** Train ***********\")\n",
    "\n",
    "    # Epoch training\n",
    "    for epoch in range(n_epochs):\n",
    "        _, error = sess.run([optimizer, err], feed_dict={X: features_train, Y: labels_train})\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch:\", epoch, \" Error:\", error)\n",
    "\n",
    "    print(\"*********** Test ***********\")\n",
    "\n",
    "    correct = tf.equal(tf.argmax(op, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "    print('Accuracy:', accuracy.eval({X: features_test, Y: labels_test}) * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
