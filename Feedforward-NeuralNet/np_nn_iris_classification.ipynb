{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network\n",
    "###### iris classification using numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('iris-data.txt', <http.client.HTTPMessage at 0x7fa0b0f89d68>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", \n",
    "    \"iris-data.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seed random-generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_ratio = 0.8\n",
    "\n",
    "tmp_list = []\n",
    "tmp_set = set()\n",
    "features = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text-file to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"iris-data.txt\") as f:\n",
    "    for line in f.readlines():\n",
    "        if not line.isspace():\n",
    "            tmp_list.append(line)\n",
    "\n",
    "    random.shuffle(tmp_list)\n",
    "\n",
    "for line in tmp_list:\n",
    "    split_line = line.strip().split(',')\n",
    "    length_line = len(split_line)\n",
    "\n",
    "    for i in range(length_line - 1):\n",
    "        split_line[i] = float(split_line[i])\n",
    "\n",
    "    label = split_line[length_line - 1]\n",
    "    tmp_set.add(label)\n",
    "\n",
    "    features.append(split_line[:length_line - 1])\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_val = max([item for i in features for item in i])\n",
    "min_val = min([item for i in features for item in i])\n",
    "\n",
    "for i in range(len(features)):\n",
    "    for j in range(len(features[0])):\n",
    "        features[i][j] = (features[i][j] - min_val) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_list = list(tmp_set)\n",
    "for i in range(len(labels)):\n",
    "    labels[i] = tmp_list.index(labels[i])\n",
    "\n",
    "label_idx = np.array(labels)\n",
    "labels = np.zeros((len(labels), len(tmp_list)))\n",
    "labels[np.arange(len(labels)), label_idx] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split into train-test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train = np.array(features[:int(train_test_ratio * len(features))])\n",
    "features_test = np.array(features[int(train_test_ratio * len(features)):])\n",
    "\n",
    "labels_train = labels[:int(train_test_ratio * len(labels))]\n",
    "labels_test = labels[int(train_test_ratio * len(labels)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_input_layers = len(features_test[0])\n",
    "n_hidden_layers = 5\n",
    "n_output_layers = len(tmp_list)\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "n_epoch = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation Functions and their derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activation_f = {\n",
    "    'identity': lambda x: x,\n",
    "    'sigmoid': lambda x: 1.0 / (1.0 + np.exp(-x)),\n",
    "    'tanh': lambda x: np.tanh(x),\n",
    "    'relu': lambda x: x * (x > 0),\n",
    "}\n",
    "\n",
    "activation_f_prime = {\n",
    "    'identity': lambda x: 1,\n",
    "    'sigmoid': lambda x: x * (1.0 - x),\n",
    "    'tanh': lambda x: 1 - x**2,\n",
    "    'relu': lambda x: 1.0 * (x > 0),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation Function Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1 = 'tanh'\n",
    "f2 = 'sigmoid'\n",
    "\n",
    "act_f1 = activation_f[f1]\n",
    "act_f2 = activation_f[f2]\n",
    "\n",
    "act_f1_prime = activation_f_prime[f1]\n",
    "act_f2_prime = activation_f_prime[f2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(input_features, output_label, i_h_weights, h_o_weights):\n",
    "    input_features = input_features.reshape(1, -1)\n",
    "\n",
    "    # forward prop\n",
    "    h_inter = np.dot(input_features, i_h_weights)\n",
    "    h_result = act_f1(h_inter)\n",
    "    o_inter = np.dot(h_result, h_o_weights)\n",
    "    o_result = act_f2(o_inter)\n",
    "\n",
    "    error = np.mean(0.5 * np.square(o_result - output_label))\n",
    "\n",
    "    # back prop\n",
    "    del_h_o = -np.multiply(output_label - o_result, act_f2_prime(o_result))\n",
    "    change_h_o = np.dot(h_result.T, del_h_o)\n",
    "    del_i_h = np.dot(del_h_o, h_o_weights.T) * act_f1_prime(h_result)\n",
    "    change_i_h = np.dot(input_features.T, del_i_h)\n",
    "\n",
    "    return error, change_i_h, change_h_o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uses just forward prop\n",
    "def predict(input_features, i_h_weights, h_o_weights):\n",
    "    h_inter = np.dot(input_features, i_h_weights)\n",
    "    h_result = act_f1(h_inter)\n",
    "    o_inter = np.dot(h_result, h_o_weights)\n",
    "    o_result = act_f2(o_inter)\n",
    "    return (o_result >= max(o_result)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** Train ***********\n",
      "Epoch: 0  Train-error: 0.110355817438  Validation-error: 0.0890507943417\n",
      "Epoch: 10  Train-error: 0.0426840919513  Validation-error: 0.0397109277993\n",
      "Epoch: 20  Train-error: 0.0221095706162  Validation-error: 0.020960218994\n",
      "Epoch: 30  Train-error: 0.016501428106  Validation-error: 0.0114677158671\n",
      "Epoch: 40  Train-error: 0.0143184820028  Validation-error: 0.0135157142136\n",
      "Epoch: 50  Train-error: 0.0120528834448  Validation-error: 0.00838103175258\n",
      "Epoch: 60  Train-error: 0.00950958946663  Validation-error: 0.0203980764745\n",
      "Epoch: 70  Train-error: 0.00861524656063  Validation-error: 0.0084052920011\n",
      "Epoch: 80  Train-error: 0.0084945926808  Validation-error: 0.00795761149001\n",
      "Epoch: 90  Train-error: 0.00819203245711  Validation-error: 0.00802635459742\n"
     ]
    }
   ],
   "source": [
    "print(\"*********** Train ***********\")\n",
    "\n",
    "# Initial Random Weights\n",
    "V = np.random.normal(scale=0.1, size=(n_input_layers, n_hidden_layers))\n",
    "W = np.random.normal(scale=0.1, size=(n_hidden_layers, n_output_layers))\n",
    "\n",
    "# Training-set\n",
    "X = features_train\n",
    "T = labels_train\n",
    "\n",
    "# Epoch-training\n",
    "for epoch in range(n_epoch):\n",
    "    tr_err = []\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        loss, grad_V, grad_W = train(X[i], T[i], V, W)\n",
    "\n",
    "        # Adjust Weights\n",
    "        V -= learning_rate * grad_V + momentum * grad_V\n",
    "        W -= learning_rate * grad_W + momentum * grad_W\n",
    "\n",
    "        tr_err.append(loss)\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        val_err = []\n",
    "        \n",
    "        # use test set as validiation set\n",
    "        for i in range(features_test.shape[0]):\n",
    "            loss, _, _ = train(features_test[i], labels_test[i], V, W)\n",
    "            val_err.append(loss)\n",
    "            \n",
    "        train_error = sum(tr_err) / len(tr_err)\n",
    "        valid_error = sum(val_err) / len(val_err)\n",
    "        \n",
    "        print(\"Epoch:\", epoch, \" Train-error:\", train_error, \" Validation-error:\", valid_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** Test ***********\n",
      "Total = 30 Success = 29 Accuracy = 96.666667\n"
     ]
    }
   ],
   "source": [
    "print(\"*********** Test ***********\")\n",
    "\n",
    "success = 0\n",
    "for i in range(len(features_test)):\n",
    "    a = predict(features_test[i], V, W)\n",
    "    b = labels_test[i]\n",
    "    if np.array_equal(a, b):\n",
    "        success += 1\n",
    "\n",
    "print(\"Total = %d Success = %d Accuracy = %f\" %\n",
    "      (len(features_test), success, success * 100 / len(features_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
