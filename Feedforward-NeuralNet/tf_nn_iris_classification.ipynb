{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network\n",
    "###### iris classification using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('iris-data.txt', <http.client.HTTPMessage at 0x7ff22a7cadd8>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", \n",
    "    \"iris-data.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seed random-generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_ratio = 0.8\n",
    "\n",
    "tmp_list = []\n",
    "tmp_set = set()\n",
    "features = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text-file to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"iris-data.txt\") as f:\n",
    "    for line in f.readlines():\n",
    "        if not line.isspace():\n",
    "            tmp_list.append(line)\n",
    "\n",
    "    random.shuffle(tmp_list)\n",
    "\n",
    "for line in tmp_list:\n",
    "    split_line = line.strip().split(',')\n",
    "    length_line = len(split_line)\n",
    "\n",
    "    for i in range(length_line - 1):\n",
    "        split_line[i] = float(split_line[i])\n",
    "\n",
    "    label = split_line[length_line - 1]\n",
    "    tmp_set.add(label)\n",
    "\n",
    "    features.append(split_line[:length_line - 1])\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_val = max([item for i in features for item in i])\n",
    "min_val = min([item for i in features for item in i])\n",
    "\n",
    "for i in range(len(features)):\n",
    "    for j in range(len(features[0])):\n",
    "        features[i][j] = (features[i][j] - min_val) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_list = list(tmp_set)\n",
    "for i in range(len(labels)):\n",
    "    labels[i] = tmp_list.index(labels[i])\n",
    "\n",
    "label_idx = np.array(labels)\n",
    "labels = np.zeros((len(labels), len(tmp_list)))\n",
    "labels[np.arange(len(labels)), label_idx] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split into train-test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train = np.array(features[:int(train_test_ratio * len(features))])\n",
    "features_test = np.array(features[int(train_test_ratio * len(features)):])\n",
    "\n",
    "labels_train = labels[:int(train_test_ratio * len(labels))]\n",
    "labels_test = labels[int(train_test_ratio * len(labels)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_input_layers = len(features_test[0])\n",
    "n_hidden_layers_1 = 5\n",
    "n_output_layers = len(tmp_list)\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input/output placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, n_input_layers])\n",
    "Y = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_1 = {\n",
    "    'weights': tf.Variable(tf.random_normal([n_input_layers, n_hidden_layers_1], stddev=0.1)),\n",
    "    'biases': tf.Variable(tf.random_normal([n_hidden_layers_1], stddev=0.1))\n",
    "}\n",
    "layer_op = {\n",
    "    'weights': tf.Variable(tf.random_normal([n_hidden_layers_1, n_output_layers], stddev=0.1)),\n",
    "    'biases': tf.Variable(tf.random_normal([n_output_layers], stddev=0.1))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_l1 = tf.nn.xw_plus_b(X, layer_1['weights'], layer_1['biases'])\n",
    "l1 = tf.nn.tanh(h_l1)\n",
    "\n",
    "h_l2 = tf.nn.xw_plus_b(l1, layer_op['weights'], layer_op['biases'])\n",
    "op = tf.nn.sigmoid(h_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mean-squared error\n",
    "err = tf.losses.mean_squared_error(predictions=op, labels=Y)\n",
    "\n",
    "# gradient-descent-with-momentum-optimizer\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum).minimize(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** Train ***********\n",
      "Epoch: 0  Train-error: 0.238889564698  Validation-error: 0.230705\n",
      "Epoch: 10  Train-error: 0.175986521132  Validation-error: 0.175288\n",
      "Epoch: 20  Train-error: 0.112499419212  Validation-error: 0.123292\n",
      "Epoch: 30  Train-error: 0.101820785125  Validation-error: 0.112566\n",
      "Epoch: 40  Train-error: 0.0957876971224  Validation-error: 0.106434\n",
      "Epoch: 50  Train-error: 0.0907092668271  Validation-error: 0.10041\n",
      "Epoch: 60  Train-error: 0.0834965036406  Validation-error: 0.0900948\n",
      "Epoch: 70  Train-error: 0.0723732309804  Validation-error: 0.0751268\n",
      "Epoch: 80  Train-error: 0.0587168289116  Validation-error: 0.0580571\n",
      "Epoch: 90  Train-error: 0.0461099247953  Validation-error: 0.0434318\n",
      "*********** Test ***********\n",
      "Test-error: 0.0342728 Accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    print(\"*********** Train ***********\")\n",
    "\n",
    "    # Epoch training\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        tr_err = []\n",
    "\n",
    "        for i in range(len(features_train)):\n",
    "            _, error = sess.run([optimizer, err], feed_dict={\n",
    "                    X: features_train[i].reshape(1, -1),\n",
    "                    Y: labels_train[i]\n",
    "                })\n",
    "            tr_err.append(error)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            # use test set for validation\n",
    "            val_err = err.eval({X: features_test, Y: labels_test})\n",
    "            train_err = sum(tr_err) / len(tr_err)\n",
    "            print(\"Epoch:\", epoch, \" Train-error:\", train_err, \" Validation-error:\", val_err)\n",
    "\n",
    "    print(\"*********** Test ***********\")\n",
    "\n",
    "    correct = tf.equal(tf.argmax(op, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "    test_error = err.eval({X: features_test, Y: labels_test})\n",
    "    test_accuracy = accuracy.eval({X: features_test, Y: labels_test}) * 100\n",
    "    print('Test-error:', test_error, 'Accuracy:', test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
